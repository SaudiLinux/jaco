#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Web Penetration Testing Tool
Developed by: Saudi Linux
Email: SaudiLinux7@gmail.com

This tool is designed for educational purposes and authorized penetration testing only.
Using this tool against websites without explicit permission may be illegal.
"""

import argparse
import sys
import os
import time
import json
import random
from datetime import datetime

try:
    import requests
    from bs4 import BeautifulSoup
    import colorama
    from colorama import Fore, Style
    import urllib3
    from fake_useragent import UserAgent
    import whois
    import dns.resolver
    from urllib.parse import urlparse, urljoin
except ImportError as e:
    print(f"Error: Missing required dependencies. {e}")
    print("Please run: pip install -r requirements.txt")
    sys.exit(1)

# Disable SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Initialize colorama
colorama.init(autoreset=True)

# Banner
def print_banner():
    banner = f'''
    {Fore.GREEN}╔══════════════════════════════════════════════════════════╗
    ║                                                          ║
    ║  {Fore.RED}██████╗ ███████╗███╗   ██╗████████╗███████╗███████╗{Fore.GREEN}  ║
    ║  {Fore.RED}██╔══██╗██╔════╝████╗  ██║╚══██╔══╝██╔════╝██╔════╝{Fore.GREEN}  ║
    ║  {Fore.RED}██████╔╝█████╗  ██╔██╗ ██║   ██║   █████╗  ███████╗{Fore.GREEN}  ║
    ║  {Fore.RED}██╔═══╝ ██╔══╝  ██║╚██╗██║   ██║   ██╔══╝  ╚════██║{Fore.GREEN}  ║
    ║  {Fore.RED}██║     ███████╗██║ ╚████║   ██║   ███████╗███████║{Fore.GREEN}  ║
    ║  {Fore.RED}╚═╝     ╚══════╝╚═╝  ╚═══╝   ╚═╝   ╚══════╝╚══════╝{Fore.GREEN}  ║
    ║                                                          ║
    ║  {Fore.CYAN}Web Penetration Testing Tool v1.0{Fore.GREEN}                   ║
    ║  {Fore.CYAN}Developed by: Saudi Linux{Fore.GREEN}                          ║
    ║  {Fore.CYAN}Email: SaudiLinux7@gmail.com{Fore.GREEN}                       ║
    ║                                                          ║
    ╚══════════════════════════════════════════════════════════╝{Style.RESET_ALL}
    '''
    print(banner)

# User-Agent rotation
def get_random_user_agent():
    try:
        ua = UserAgent()
        return ua.random
    except:
        # Fallback user agents if fake-useragent fails
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 11.5; rv:90.0) Gecko/20100101 Firefox/90.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_5_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15',
        ]
        return random.choice(user_agents)

# Request wrapper with error handling
def make_request(url, method="GET", data=None, headers=None, timeout=10, allow_redirects=True, verify=False):
    if headers is None:
        headers = {
            'User-Agent': get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
    
    try:
        if method.upper() == "GET":
            response = requests.get(
                url, 
                headers=headers, 
                timeout=timeout, 
                allow_redirects=allow_redirects,
                verify=verify
            )
        elif method.upper() == "POST":
            response = requests.post(
                url, 
                data=data, 
                headers=headers, 
                timeout=timeout, 
                allow_redirects=allow_redirects,
                verify=verify
            )
        elif method.upper() == "HEAD":
            response = requests.head(
                url, 
                headers=headers, 
                timeout=timeout, 
                allow_redirects=allow_redirects,
                verify=verify
            )
        else:
            raise ValueError(f"Unsupported HTTP method: {method}")
            
        return response
    except requests.exceptions.ConnectionError:
        print(f"{Fore.RED}[!] Connection Error: Could not connect to {url}{Style.RESET_ALL}")
    except requests.exceptions.Timeout:
        print(f"{Fore.RED}[!] Timeout Error: Request to {url} timed out{Style.RESET_ALL}")
    except requests.exceptions.RequestException as e:
        print(f"{Fore.RED}[!] Request Error: {e}{Style.RESET_ALL}")
    except Exception as e:
        print(f"{Fore.RED}[!] Error: {e}{Style.RESET_ALL}")
    
    return None

# Metadata extraction
class MetadataExtractor:
    def __init__(self, url, verbose=False):
        self.url = url
        self.verbose = verbose
        self.results = {}
        self.domain = urlparse(url).netloc
    
    def extract_all_metadata(self):
        print(f"\n{Fore.CYAN}[*] Extracting metadata from {self.url}{Style.RESET_ALL}")
        
        self.extract_http_headers()
        self.extract_dns_info()
        self.extract_whois_info()
        self.extract_html_metadata()
        self.extract_server_info()
        
        return self.results
    
    def extract_http_headers(self):
        print(f"{Fore.YELLOW}[+] Extracting HTTP headers...{Style.RESET_ALL}")
        response = make_request(self.url, method="HEAD")
        
        if response:
            self.results['http_headers'] = dict(response.headers)
            
            # Check for security headers
            security_headers = {
                'Strict-Transport-Security': 'Missing HSTS header',
                'Content-Security-Policy': 'Missing CSP header',
                'X-Content-Type-Options': 'Missing X-Content-Type-Options header',
                'X-Frame-Options': 'Missing X-Frame-Options header',
                'X-XSS-Protection': 'Missing X-XSS-Protection header',
                'Referrer-Policy': 'Missing Referrer-Policy header',
                'Permissions-Policy': 'Missing Permissions-Policy header',
            }
            
            missing_headers = []
            for header, message in security_headers.items():
                if header not in response.headers:
                    missing_headers.append(message)
            
            if missing_headers:
                self.results['missing_security_headers'] = missing_headers
                if self.verbose:
                    for msg in missing_headers:
                        print(f"{Fore.RED}[!] {msg}{Style.RESET_ALL}")
    
    def extract_dns_info(self):
        print(f"{Fore.YELLOW}[+] Extracting DNS information...{Style.RESET_ALL}")
        dns_records = {}
        
        try:
            # A records
            try:
                answers = dns.resolver.resolve(self.domain, 'A')
                dns_records['A'] = [answer.address for answer in answers]
            except Exception:
                dns_records['A'] = []
            
            # MX records
            try:
                answers = dns.resolver.resolve(self.domain, 'MX')
                dns_records['MX'] = [str(answer.exchange) for answer in answers]
            except Exception:
                dns_records['MX'] = []
            
            # NS records
            try:
                answers = dns.resolver.resolve(self.domain, 'NS')
                dns_records['NS'] = [str(answer) for answer in answers]
            except Exception:
                dns_records['NS'] = []
            
            # TXT records
            try:
                answers = dns.resolver.resolve(self.domain, 'TXT')
                dns_records['TXT'] = [str(answer) for answer in answers]
            except Exception:
                dns_records['TXT'] = []
            
            self.results['dns_records'] = dns_records
            
            if self.verbose:
                for record_type, records in dns_records.items():
                    if records:
                        print(f"{Fore.GREEN}[+] {record_type} Records: {', '.join(records)}{Style.RESET_ALL}")
        
        except Exception as e:
            print(f"{Fore.RED}[!] Error extracting DNS information: {e}{Style.RESET_ALL}")
    
    def extract_whois_info(self):
        print(f"{Fore.YELLOW}[+] Extracting WHOIS information...{Style.RESET_ALL}")
        try:
            whois_info = whois.whois(self.domain)
            
            # Extract relevant WHOIS data
            relevant_whois = {
                'registrar': whois_info.registrar,
                'creation_date': str(whois_info.creation_date),
                'expiration_date': str(whois_info.expiration_date),
                'name_servers': whois_info.name_servers,
            }
            
            self.results['whois_info'] = relevant_whois
            
            if self.verbose:
                print(f"{Fore.GREEN}[+] Registrar: {whois_info.registrar}{Style.RESET_ALL}")
                print(f"{Fore.GREEN}[+] Creation Date: {whois_info.creation_date}{Style.RESET_ALL}")
                print(f"{Fore.GREEN}[+] Expiration Date: {whois_info.expiration_date}{Style.RESET_ALL}")
        
        except Exception as e:
            print(f"{Fore.RED}[!] Error extracting WHOIS information: {e}{Style.RESET_ALL}")
    
    def extract_html_metadata(self):
        print(f"{Fore.YELLOW}[+] Extracting HTML metadata...{Style.RESET_ALL}")
        response = make_request(self.url)
        
        if response:
            try:
                soup = BeautifulSoup(response.text, 'html.parser')
                meta_tags = {}
                
                # Extract meta tags
                for meta in soup.find_all('meta'):
                    name = meta.get('name') or meta.get('property')
                    content = meta.get('content')
                    
                    if name and content:
                        meta_tags[name] = content
                
                # Extract generator info
                generator = soup.find('meta', attrs={'name': 'generator'})
                if generator:
                    self.results['cms'] = generator.get('content')
                
                # Extract links
                links = []
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if href.startswith('http') or href.startswith('https'):
                        links.append(href)
                    else:
                        # Convert relative URLs to absolute
                        absolute_url = urljoin(self.url, href)
                        links.append(absolute_url)
                
                # Extract scripts
                scripts = [script.get('src') for script in soup.find_all('script', src=True)]
                
                # Extract comments
                comments = []
                for comment in soup.find_all(string=lambda text: isinstance(text, str) and text.strip().startswith('<!--')):
                    comments.append(comment.strip())
                
                self.results['meta_tags'] = meta_tags
                self.results['links'] = links[:50]  # Limit to 50 links
                self.results['scripts'] = scripts
                self.results['comments'] = comments
                
                if self.verbose:
                    print(f"{Fore.GREEN}[+] Found {len(meta_tags)} meta tags{Style.RESET_ALL}")
                    print(f"{Fore.GREEN}[+] Found {len(links)} links{Style.RESET_ALL}")
                    print(f"{Fore.GREEN}[+] Found {len(scripts)} scripts{Style.RESET_ALL}")
                    print(f"{Fore.GREEN}[+] Found {len(comments)} comments{Style.RESET_ALL}")
            
            except Exception as e:
                print(f"{Fore.RED}[!] Error parsing HTML: {e}{Style.RESET_ALL}")
    
    def extract_server_info(self):
        print(f"{Fore.YELLOW}[+] Extracting server information...{Style.RESET_ALL}")
        response = make_request(self.url, method="HEAD")
        
        if response and 'Server' in response.headers:
            server = response.headers['Server']
            self.results['server'] = server
            
            if self.verbose:
                print(f"{Fore.GREEN}[+] Server: {server}{Style.RESET_ALL}")

# Vulnerability scanner
class VulnerabilityScanner:
    def __init__(self, url, verbose=False):
        self.url = url
        self.verbose = verbose
        self.results = {}
        self.domain = urlparse(url).netloc
    
    def scan_all(self):
        print(f"\n{Fore.CYAN}[*] Scanning for vulnerabilities on {self.url}{Style.RESET_ALL}")
        
        self.scan_xss()
        self.scan_sql_injection()
        self.scan_open_redirects()
        self.scan_csrf()
        self.scan_misconfigurations()
        
        return self.results
    
    def scan_xss(self):
        print(f"{Fore.YELLOW}[+] Scanning for XSS vulnerabilities...{Style.RESET_ALL}")
        xss_payloads = [
            '<script>alert(1)</script>',
            '"><script>alert(1)</script>',
            '\'"><script>alert(1)</script>',
            '<img src=x onerror=alert(1)>',
            '<svg onload=alert(1)>',
        ]
        
        # First, find potential injection points
        response = make_request(self.url)
        if not response:
            return
        
        soup = BeautifulSoup(response.text, 'html.parser')
        forms = soup.find_all('form')
        
        potential_xss = []
        
        # Test forms
        for form in forms:
            form_action = form.get('action', '')
            if not form_action:
                form_action = self.url
            elif not form_action.startswith(('http://', 'https://')):
                form_action = urljoin(self.url, form_action)
            
            method = form.get('method', 'get').lower()
            inputs = form.find_all(['input', 'textarea'])
            
            for payload in xss_payloads:
                data = {}
                for input_field in inputs:
                    input_name = input_field.get('name')
                    if input_name:
                        data[input_name] = payload
                
                if method == 'get':
                    # Construct URL with parameters
                    query_string = '&'.join([f"{k}={v}" for k, v in data.items()])
                    test_url = f"{form_action}?{query_string}"
                    test_response = make_request(test_url)
                else:  # POST
                    test_response = make_request(form_action, method="POST", data=data)
                
                if test_response and payload in test_response.text:
                    potential_xss.append({
                        'url': form_action,
                        'method': method,
                        'payload': payload,
                        'form_index': forms.index(form)
                    })
                    break  # Found a working payload for this form
        
        # Test URL parameters
        parsed_url = urlparse(self.url)
        if parsed_url.query:
            query_params = parsed_url.query.split('&')
            for param in query_params:
                if '=' in param:
                    param_name = param.split('=')[0]
                    for payload in xss_payloads:
                        # Replace this parameter with our payload
                        new_query = parsed_url.query.replace(param, f"{param_name}={payload}")
                        test_url = self.url.replace(parsed_url.query, new_query)
                        
                        test_response = make_request(test_url)
                        if test_response and payload in test_response.text:
                            potential_xss.append({
                                'url': test_url,
                                'method': 'get',
                                'payload': payload,
                                'parameter': param_name
                            })
                            break  # Found a working payload for this parameter
        
        if potential_xss:
            self.results['xss_vulnerabilities'] = potential_xss
            print(f"{Fore.RED}[!] Found {len(potential_xss)} potential XSS vulnerabilities{Style.RESET_ALL}")
            
            if self.verbose:
                for vuln in potential_xss:
                    print(f"{Fore.RED}[!] Potential XSS at {vuln['url']} using {vuln['method'].upper()} method{Style.RESET_ALL}")
        else:
            print(f"{Fore.GREEN}[+] No XSS vulnerabilities found{Style.RESET_ALL}")
    
    def scan_sql_injection(self):
        print(f"{Fore.YELLOW}[+] Scanning for SQL Injection vulnerabilities...{Style.RESET_ALL}")
        sqli_payloads = [
            "' OR '1'='1",
            "\" OR \"1\"=\"1",
            "1' OR '1'='1' --",
            "1\" OR \"1\"=\"1\" --",
            "' OR 1=1 --",
            "\" OR 1=1 --",
            "' OR '1'='1' /*",
            "\" OR \"1\"=\"1\" /*",
        ]
        
        error_patterns = [
            "SQL syntax",
            "mysql_fetch",
            "ORA-",
            "Oracle error",
            "Microsoft SQL Server",
            "PostgreSQL",
            "SQLite",
            "syntax error",
            "unclosed quotation mark",
        ]
        
        # First, find potential injection points
        response = make_request(self.url)
        if not response:
            return
        
        soup = BeautifulSoup(response.text, 'html.parser')
        forms = soup.find_all('form')
        
        potential_sqli = []
        
        # Test forms
        for form in forms:
            form_action = form.get('action', '')
            if not form_action:
                form_action = self.url
            elif not form_action.startswith(('http://', 'https://')):
                form_action = urljoin(self.url, form_action)
            
            method = form.get('method', 'get').lower()
            inputs = form.find_all(['input', 'textarea'])
            
            for payload in sqli_payloads:
                data = {}
                for input_field in inputs:
                    input_name = input_field.get('name')
                    if input_name:
                        data[input_name] = payload
                
                if method == 'get':
                    # Construct URL with parameters
                    query_string = '&'.join([f"{k}={v}" for k, v in data.items()])
                    test_url = f"{form_action}?{query_string}"
                    test_response = make_request(test_url)
                else:  # POST
                    test_response = make_request(form_action, method="POST", data=data)
                
                if test_response:
                    # Check for SQL error messages
                    for pattern in error_patterns:
                        if pattern in test_response.text:
                            potential_sqli.append({
                                'url': form_action,
                                'method': method,
                                'payload': payload,
                                'error_pattern': pattern,
                                'form_index': forms.index(form)
                            })
                            break  # Found a working payload for this form
        
        # Test URL parameters
        parsed_url = urlparse(self.url)
        if parsed_url.query:
            query_params = parsed_url.query.split('&')
            for param in query_params:
                if '=' in param:
                    param_name = param.split('=')[0]
                    for payload in sqli_payloads:
                        # Replace this parameter with our payload
                        new_query = parsed_url.query.replace(param, f"{param_name}={payload}")
                        test_url = self.url.replace(parsed_url.query, new_query)
                        
                        test_response = make_request(test_url)
                        if test_response:
                            # Check for SQL error messages
                            for pattern in error_patterns:
                                if pattern in test_response.text:
                                    potential_sqli.append({
                                        'url': test_url,
                                        'method': 'get',
                                        'payload': payload,
                                        'error_pattern': pattern,
                                        'parameter': param_name
                                    })
                                    break  # Found a working payload for this parameter
        
        if potential_sqli:
            self.results['sqli_vulnerabilities'] = potential_sqli
            print(f"{Fore.RED}[!] Found {len(potential_sqli)} potential SQL Injection vulnerabilities{Style.RESET_ALL}")
            
            if self.verbose:
                for vuln in potential_sqli:
                    print(f"{Fore.RED}[!] Potential SQL Injection at {vuln['url']} using {vuln['method'].upper()} method{Style.RESET_ALL}")
        else:
            print(f"{Fore.GREEN}[+] No SQL Injection vulnerabilities found{Style.RESET_ALL}")
    
    def scan_open_redirects(self):
        print(f"{Fore.YELLOW}[+] Scanning for Open Redirect vulnerabilities...{Style.RESET_ALL}")
        redirect_payloads = [
            "https://evil.com",
            "//evil.com",
            "/\\evil.com",
            "https:evil.com",
            "https://evil.com%2523.target.com",
        ]
        
        # Find potential redirect parameters
        redirect_params = ['redirect', 'url', 'next', 'redir', 'return', 'to', 'goto', 'link', 'target']
        
        # First, find potential injection points
        response = make_request(self.url)
        if not response:
            return
        
        potential_redirects = []
        
        # Test URL parameters
        parsed_url = urlparse(self.url)
        if parsed_url.query:
            query_params = parsed_url.query.split('&')
            for param in query_params:
                if '=' in param:
                    param_name, param_value = param.split('=')
                    if param_name.lower() in redirect_params:
                        for payload in redirect_payloads:
                            # Replace this parameter with our payload
                            new_query = parsed_url.query.replace(param, f"{param_name}={payload}")
                            test_url = self.url.replace(parsed_url.query, new_query)
                            
                            test_response = make_request(test_url, allow_redirects=False)
                            if test_response and (test_response.status_code in [301, 302, 303, 307, 308]):
                                location = test_response.headers.get('Location', '')
                                if payload in location or 'evil.com' in location:
                                    potential_redirects.append({
                                        'url': test_url,
                                        'payload': payload,
                                        'parameter': param_name,
                                        'redirect_to': location
                                    })
                                    break  # Found a working payload for this parameter
        
        # Also check for forms with potential redirect fields
        soup = BeautifulSoup(response.text, 'html.parser')
        forms = soup.find_all('form')
        
        for form in forms:
            form_action = form.get('action', '')
            if not form_action:
                form_action = self.url
            elif not form_action.startswith(('http://', 'https://')):
                form_action = urljoin(self.url, form_action)
            
            method = form.get('method', 'get').lower()
            inputs = form.find_all(['input', 'textarea'])
            
            for input_field in inputs:
                input_name = input_field.get('name', '').lower()
                if input_name in redirect_params:
                    for payload in redirect_payloads:
                        data = {}
                        for inp in inputs:
                            inp_name = inp.get('name')
                            if inp_name:
                                if inp_name.lower() == input_name:
                                    data[inp_name] = payload
                                else:
                                    data[inp_name] = inp.get('value', '')
                        
                        if method == 'get':
                            # Construct URL with parameters
                            query_string = '&'.join([f"{k}={v}" for k, v in data.items()])
                            test_url = f"{form_action}?{query_string}"
                            test_response = make_request(test_url, allow_redirects=False)
                        else:  # POST
                            test_response = make_request(form_action, method="POST", data=data, allow_redirects=False)
                        
                        if test_response and (test_response.status_code in [301, 302, 303, 307, 308]):
                            location = test_response.headers.get('Location', '')
                            if payload in location or 'evil.com' in location:
                                potential_redirects.append({
                                    'url': form_action,
                                    'method': method,
                                    'payload': payload,
                                    'parameter': input_name,
                                    'redirect_to': location,
                                    'form_index': forms.index(form)
                                })
                                break  # Found a working payload for this form
        
        if potential_redirects:
            self.results['open_redirect_vulnerabilities'] = potential_redirects
            print(f"{Fore.RED}[!] Found {len(potential_redirects)} potential Open Redirect vulnerabilities{Style.RESET_ALL}")
            
            if self.verbose:
                for vuln in potential_redirects:
                    print(f"{Fore.RED}[!] Potential Open Redirect at {vuln['url']} via parameter {vuln['parameter']}{Style.RESET_ALL}")
        else:
            print(f"{Fore.GREEN}[+] No Open Redirect vulnerabilities found{Style.RESET_ALL}")
    
    def scan_csrf(self):
        print(f"{Fore.YELLOW}[+] Scanning for CSRF vulnerabilities...{Style.RESET_ALL}")
        
        # First, find forms
        response = make_request(self.url)
        if not response:
            return
        
        soup = BeautifulSoup(response.text, 'html.parser')
        forms = soup.find_all('form')
        
        potential_csrf = []
        
        for form in forms:
            # Check if the form has CSRF protection
            csrf_protected = False
            
            # Check for CSRF token in hidden inputs
            hidden_inputs = form.find_all('input', {'type': 'hidden'})
            for hidden_input in hidden_inputs:
                input_name = hidden_input.get('name', '').lower()
                if any(token_name in input_name for token_name in ['csrf', 'token', 'nonce', 'xsrf']):
                    csrf_protected = True
                    break
            
            # Check for CSRF token in form attributes
            form_attrs = form.attrs
            for attr_name, attr_value in form_attrs.items():
                if isinstance(attr_value, str) and any(token_name in attr_name.lower() for token_name in ['csrf', 'token', 'nonce', 'xsrf']):
                    csrf_protected = True
                    break
            
            if not csrf_protected:
                form_action = form.get('action', '')
                if not form_action:
                    form_action = self.url
                elif not form_action.startswith(('http://', 'https://')):
                    form_action = urljoin(self.url, form_action)
                
                method = form.get('method', 'get').lower()
                
                # Only POST forms are typically vulnerable to CSRF
                if method.lower() == 'post':
                    potential_csrf.append({
                        'url': form_action,
                        'method': method,
                        'form_index': forms.index(form)
                    })
        
        if potential_csrf:
            self.results['csrf_vulnerabilities'] = potential_csrf
            print(f"{Fore.RED}[!] Found {len(potential_csrf)} potential CSRF vulnerabilities{Style.RESET_ALL}")
            
            if self.verbose:
                for vuln in potential_csrf:
                    print(f"{Fore.RED}[!] Potential CSRF vulnerability in form at {vuln['url']} using {vuln['method'].upper()} method{Style.RESET_ALL}")
        else:
            print(f"{Fore.GREEN}[+] No CSRF vulnerabilities found{Style.RESET_ALL}")
    
    def scan_misconfigurations(self):
        print(f"{Fore.YELLOW}[+] Scanning for security misconfigurations...{Style.RESET_ALL}")
        
        misconfigurations = []
        
        # Check for directory listing
        common_dirs = ['admin/', 'backup/', 'config/', 'db/', 'logs/', 'test/']
        for directory in common_dirs:
            test_url = urljoin(self.url, directory)
            response = make_request(test_url)
            
            if response and response.status_code == 200:
                # Check for common directory listing signatures
                if any(signature in response.text for signature in ['Index of /', 'Directory Listing For', 'Parent Directory']):
                    misconfigurations.append({
                        'type': 'directory_listing',
                        'url': test_url,
                        'description': f"Directory listing enabled at {test_url}"
                    })
                    if self.verbose:
                        print(f"{Fore.RED}[!] Directory listing enabled at {test_url}{Style.RESET_ALL}")
        
        # Check for default files
        default_files = ['phpinfo.php', 'test.php', 'info.php', 'server-status', 'server-info', 'robots.txt', '.git/HEAD']
        for file in default_files:
            test_url = urljoin(self.url, file)
            response = make_request(test_url)
            
            if response and response.status_code == 200:
                if file == 'phpinfo.php' or file == 'info.php':
                    if 'PHP Version' in response.text and 'PHP Credits' in response.text:
                        misconfigurations.append({
                            'type': 'information_disclosure',
                            'url': test_url,
                            'description': f"PHP info page accessible at {test_url}"
                        })
                        if self.verbose:
                            print(f"{Fore.RED}[!] PHP info page accessible at {test_url}{Style.RESET_ALL}")
                
                elif file == '.git/HEAD':
                    if 'ref:' in response.text:
                        misconfigurations.append({
                            'type': 'git_repository_exposure',
                            'url': test_url,
                            'description': f"Git repository exposed at {self.url}"
                        })
                        if self.verbose:
                            print(f"{Fore.RED}[!] Git repository exposed at {self.url}{Style.RESET_ALL}")
                
                elif file == 'robots.txt':
                    # Check for sensitive paths in robots.txt
                    sensitive_paths = ['admin', 'backup', 'config', 'db', 'logs', 'private']
                    for path in sensitive_paths:
                        if path in response.text:
                            misconfigurations.append({
                                'type': 'sensitive_information_in_robots',
                                'url': test_url,
                                'description': f"Sensitive path '{path}' found in robots.txt"
                            })
                            if self.verbose:
                                print(f"{Fore.RED}[!] Sensitive path '{path}' found in robots.txt{Style.RESET_ALL}")
        
        # Check for CORS misconfiguration
        headers = {
            'Origin': 'https://evil.com',
            'User-Agent': get_random_user_agent(),
        }
        
        response = make_request(self.url, headers=headers)
        if response:
            cors_headers = {
                'Access-Control-Allow-Origin': response.headers.get('Access-Control-Allow-Origin'),
                'Access-Control-Allow-Credentials': response.headers.get('Access-Control-Allow-Credentials')
            }
            
            if cors_headers['Access-Control-Allow-Origin'] == 'https://evil.com' or cors_headers['Access-Control-Allow-Origin'] == '*':
                if cors_headers['Access-Control-Allow-Credentials'] == 'true':
                    misconfigurations.append({
                        'type': 'cors_misconfiguration',
                        'url': self.url,
                        'description': "CORS misconfiguration allows credentials to be sent to unauthorized origins",
                        'cors_headers': cors_headers
                    })
                    if self.verbose:
                        print(f"{Fore.RED}[!] CORS misconfiguration allows credentials to be sent to unauthorized origins{Style.RESET_ALL}")
        
        if misconfigurations:
            self.results['misconfigurations'] = misconfigurations
            print(f"{Fore.RED}[!] Found {len(misconfigurations)} security misconfigurations{Style.RESET_ALL}")
        else:
            print(f"{Fore.GREEN}[+] No security misconfigurations found{Style.RESET_ALL}")

# Main function
def main():
    parser = argparse.ArgumentParser(description='Web Penetration Testing Tool')
    parser.add_argument('-u', '--url', help='Target URL', required=True)
    parser.add_argument('-o', '--output', help='Output file for results')
    parser.add_argument('-v', '--verbose', help='Verbose output', action='store_true')
    parser.add_argument('-m', '--metadata', help='Extract metadata only', action='store_true')
    parser.add_argument('-s', '--scan', help='Specify scan type (all, xss, sqli, csrf)', default='all')
    
    args = parser.parse_args()
    
    # Validate URL
    if not args.url.startswith(('http://', 'https://')):
        args.url = 'http://' + args.url
    
    print_banner()
    
    print(f"{Fore.CYAN}[*] Target: {args.url}{Style.RESET_ALL}")
    print(f"{Fore.CYAN}[*] Scan started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}{Style.RESET_ALL}")
    
    # Initialize results dictionary
    results = {
        'target': args.url,
        'scan_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'metadata': {},
        'vulnerabilities': {}
    }
    
    # Extract metadata
    metadata_extractor = MetadataExtractor(args.url, args.verbose)
    results['metadata'] = metadata_extractor.extract_all_metadata()
    
    # Skip vulnerability scanning if metadata-only mode is enabled
    if not args.metadata:
        # Scan for vulnerabilities
        vulnerability_scanner = VulnerabilityScanner(args.url, args.verbose)
        
        if args.scan.lower() == 'all':
            results['vulnerabilities'] = vulnerability_scanner.scan_all()
        elif args.scan.lower() == 'xss':
            vulnerability_scanner.scan_xss()
            results['vulnerabilities']['xss_vulnerabilities'] = vulnerability_scanner.results.get('xss_vulnerabilities', [])
        elif args.scan.lower() == 'sqli':
            vulnerability_scanner.scan_sql_injection()
            results['vulnerabilities']['sqli_vulnerabilities'] = vulnerability_scanner.results.get('sqli_vulnerabilities', [])
        elif args.scan.lower() == 'csrf':
            vulnerability_scanner.scan_csrf()
            results['vulnerabilities']['csrf_vulnerabilities'] = vulnerability_scanner.results.get('csrf_vulnerabilities', [])
    
    # Print summary
    print(f"\n{Fore.CYAN}[*] Scan completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}{Style.RESET_ALL}")
    
    # Save results to file if specified
    if args.output:
        try:
            with open(args.output, 'w') as f:
                json.dump(results, f, indent=4)
            print(f"{Fore.GREEN}[+] Results saved to {args.output}{Style.RESET_ALL}")
        except Exception as e:
            print(f"{Fore.RED}[!] Error saving results: {e}{Style.RESET_ALL}")
    
    print(f"\n{Fore.GREEN}[+] Thank you for using Web Penetration Testing Tool!{Style.RESET_ALL}")
    print(f"{Fore.GREEN}[+] Developed by: Saudi Linux{Style.RESET_ALL}")
    print(f"{Fore.GREEN}[+] Email: SaudiLinux7@gmail.com{Style.RESET_ALL}")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print(f"\n{Fore.YELLOW}[!] Scan interrupted by user{Style.RESET_ALL}")
        sys.exit(0)
    except Exception as e:
        print(f"\n{Fore.RED}[!] An error occurred: {e}{Style.RESET_ALL}")
        sys.exit(1)